{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7a7faf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\irene\\OneDrive\\Desktop\\copia di copia\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COMPLETE PROJECT IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "# Standard library\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "import math\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Set, Tuple, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Third party - general\n",
    "import requests\n",
    "import urllib3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    silhouette_score,\n",
    "    calinski_harabasz_score,\n",
    "    davies_bouldin_score\n",
    ")\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import logging as transformers_logging\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Graph/Network\n",
    "import networkx as nx\n",
    "from networkx.algorithms.community import louvain_communities\n",
    "\n",
    "# Document processing\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.cleaners.core import clean_extra_whitespace\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Disable warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['TRANSFORMERS_NO_TF'] = '1'\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "transformers_logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb4402d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download: 700 papers across 7 subjects\n",
      "\n",
      " 673/700 (96.1%) - financecsscience\n",
      "673/700\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "1: This cell downloads an arXiv paper dataset by fetching 100 papers per subject \n",
    "It retrieves papers from multiple academic disciplines (mathematics, physics, computer science, statistics, economics, biology, and finance.)\n",
    "For each paper, the code downloads the PDF and saves metadata (title, authors, abstract, publication date) as a text file'''\n",
    "\n",
    "class ArxivDownloader:\n",
    "    CATEGORIES = {\n",
    "        # Mathematics\n",
    "        \"math.AG\": \"mathematics\", \"math.AT\": \"mathematics\", \"math.AP\": \"mathematics\",\n",
    "        \"math.CT\": \"mathematics\", \"math.CA\": \"mathematics\", \"math.CO\": \"mathematics\",\n",
    "        \"math.DG\": \"mathematics\", \"math.DS\": \"mathematics\", \"math.FA\": \"mathematics\",\n",
    "        \"math.GM\": \"mathematics\",\n",
    "        # Physics\n",
    "        \"physics.acc-ph\": \"physics\", \"physics.ao-ph\": \"physics\", \"physics.atom-ph\": \"physics\",\n",
    "        \"physics.atm-clus\": \"physics\", \"physics.bio-ph\": \"physics\", \"physics.chem-ph\": \"physics\",\n",
    "        \"physics.class-ph\": \"physics\", \"physics.comp-ph\": \"physics\", \"physics.data-an\": \"physics\",\n",
    "        \"physics.flu-dyn\": \"physics\", \n",
    "        \n",
    "        # Computer Science\n",
    "        \"cs.AI\": \"computer_science\", \"cs.AR\": \"computer_science\", \"cs.CC\": \"computer_science\",\n",
    "        \"cs.CE\": \"computer_science\", \"cs.CG\": \"computer_science\", \"cs.CL\": \"computer_science\",\n",
    "        \"cs.CR\": \"computer_science\", \"cs.CV\": \"computer_science\", \"cs.CY\": \"computer_science\",\n",
    "        \"cs.DB\": \"computer_science\", \n",
    "\n",
    "        # Statistics\n",
    "        \"stat.AP\": \"statistics\", \"stat.CO\": \"statistics\", \"stat.ME\": \"statistics\",\n",
    "        \"stat.ML\": \"statistics\", \"stat.OT\": \"statistics\", \"stat.TH\": \"statistics\",\n",
    "        \n",
    "        # Economics\n",
    "        \"econ.EM\": \"economics\", \"econ.GN\": \"economics\", \"econ.TH\": \"economics\",\n",
    "        \n",
    "        # Biology\n",
    "        \"q-bio.BM\": \"biology\", \"q-bio.CB\": \"biology\", \"q-bio.GN\": \"biology\",\n",
    "        \"q-bio.MN\": \"biology\", \"q-bio.NC\": \"biology\", \"q-bio.OT\": \"biology\",\n",
    "        \"q-bio.PE\": \"biology\", \"q-bio.QM\": \"biology\", \"q-bio.SC\": \"biology\",\n",
    "        \"q-bio.TO\": \"biology\",\n",
    "        \n",
    "        # Finance\n",
    "        \"q-fin.CP\": \"finance\", \"q-fin.EC\": \"finance\", \"q-fin.GN\": \"finance\",\n",
    "        \"q-fin.MF\": \"finance\", \"q-fin.PM\": \"finance\", \"q-fin.PR\": \"finance\",\n",
    "        \"q-fin.RM\": \"finance\", \"q-fin.ST\": \"finance\", \"q-fin.TR\": \"finance\",\n",
    "    }\n",
    "    \n",
    "    def __init__(self, base_dir=\"paper_dataset\"):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.base_dir.mkdir(exist_ok=True)\n",
    "        self.session = requests.Session()\n",
    "        self.session.verify = False\n",
    "\n",
    "    def download_papers(self, category: str, subject: str, limit: int = 100, progress_callback=None) -> int:\n",
    "        downloaded = 0\n",
    "        start = 0\n",
    "        batch_size = 50\n",
    "        max_attempts = limit * 3\n",
    "        attempts = 0\n",
    "        subject_dir = self.base_dir / subject\n",
    "        subject_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        while downloaded < limit and attempts < max_attempts:\n",
    "            response = self._request_batch(category, start, batch_size)\n",
    "            entries = response.text.split('<entry>')\n",
    "            \n",
    "            if len(entries) <= 1:\n",
    "                break\n",
    "\n",
    "            for entry in entries[1:]:\n",
    "                if downloaded >= limit:\n",
    "                    break\n",
    "                \n",
    "                attempts += 1\n",
    "                \n",
    "                metadata = self._extract_metadata(entry, category)\n",
    "                if not metadata:\n",
    "                    continue\n",
    "                \n",
    "                if not self._download_pdf(metadata['arxiv_id'], subject_dir):\n",
    "                    continue\n",
    "                \n",
    "                self._save_abstract(metadata, subject_dir)\n",
    "                \n",
    "                downloaded += 1\n",
    "                if progress_callback:\n",
    "                    progress_callback()\n",
    "            \n",
    "            start += batch_size\n",
    "            time.sleep(3) \n",
    "        \n",
    "        return downloaded\n",
    "    \n",
    "    def _request_batch(self, category: str, start: int, max_results: int):\n",
    "        url = \"https://export.arxiv.org/api/query\"\n",
    "        params = {\n",
    "            \"search_query\": f\"cat:{category}\",\n",
    "            \"start\": start,\n",
    "            \"max_results\": max_results,\n",
    "            \"sortBy\": \"submittedDate\",\n",
    "            \"sortOrder\": \"descending\"\n",
    "        }\n",
    "        response = self.session.get(url, params=params, timeout=30, verify=False)\n",
    "        response.raise_for_status()\n",
    "        return response\n",
    "    \n",
    "\n",
    "    def _extract_metadata(self, entry: str, category: str) -> dict:\n",
    "        id_match = re.search(r'<id>(.*?)</id>', entry)\n",
    "        title_match = re.search(r'<title>(.*?)</title>', entry, re.DOTALL)\n",
    "        summary_match = re.search(r'<summary>(.*?)</summary>', entry, re.DOTALL)\n",
    "        published_match = re.search(r'<published>(.*?)</published>', entry)\n",
    "        authors = re.findall(r'<name>(.*?)</name>', entry)\n",
    "        \n",
    "        if not (id_match and title_match and summary_match):\n",
    "            return None\n",
    "        \n",
    "        arxiv_id = id_match.group(1).split('/')[-1].split('v')[0]\n",
    "        title = self._clean_text(title_match.group(1))\n",
    "        abstract = self._clean_text(summary_match.group(1))\n",
    "        date = published_match.group(1)[:10] if published_match else \"N/A\"\n",
    "        \n",
    "        return {\n",
    "            'arxiv_id': arxiv_id,\n",
    "            'title': title,\n",
    "            'abstract': abstract,\n",
    "            'authors': authors,\n",
    "            'date': date,\n",
    "            'category': category\n",
    "        }\n",
    "    \n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        return text.strip().replace('\\n', ' ').replace('  ', ' ')\n",
    "    \n",
    "    def _download_pdf(self, arxiv_id: str, subject_dir: Path) -> bool:\n",
    "        pdf_url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
    "        \n",
    "        response = self.session.get(pdf_url, timeout=60, verify=False)\n",
    "        if response.status_code != 200:\n",
    "            return False\n",
    "        \n",
    "        filepath = subject_dir / f\"{arxiv_id}.pdf\"\n",
    "        with open(filepath, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return True\n",
    "    \n",
    "    def _save_abstract(self, metadata: dict, subject_dir: Path):\n",
    "        safe_title = re.sub(r'[^\\w\\s-]', '', metadata['title'])[:100]\n",
    "        safe_title = re.sub(r'\\s+', '_', safe_title)\n",
    "\n",
    "        filename = f\"{metadata['arxiv_id']}_{safe_title}.txt\"\n",
    "        filepath = subject_dir / filename\n",
    "        \n",
    "        content = f\"\"\"ArXiv ID: {metadata['arxiv_id']}\n",
    "Title: {metadata['title']}\n",
    "Authors: {', '.join(metadata['authors'])}\n",
    "Date: {metadata['date']}\n",
    "Category: {metadata['category']}\n",
    "URL: https://arxiv.org/abs/{metadata['arxiv_id']}\n",
    "ABSTRACT: {metadata['abstract']}\n",
    "\"\"\"\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "    \n",
    "    def download_dataset(self, papers_per_subject: int = 100):\n",
    "        subject_categories = {}\n",
    "        for cat, sub in self.CATEGORIES.items():\n",
    "            subject_categories.setdefault(sub, []).append(cat)\n",
    "        \n",
    "        total_papers = papers_per_subject * len(subject_categories)\n",
    "        downloaded_total = 0\n",
    "        \n",
    "        print(f\"Starting download: {total_papers} papers across {len(subject_categories)} subjects\\n\")\n",
    "        \n",
    "        for subject, categories in subject_categories.items():\n",
    "            papers_per_cat = papers_per_subject // len(categories)\n",
    "            extra = papers_per_subject % len(categories)\n",
    "            \n",
    "            for i, cat in enumerate(categories):\n",
    "                limit = papers_per_cat + (1 if i < extra else 0)\n",
    "                \n",
    "                def update_progress():\n",
    "                    nonlocal downloaded_total\n",
    "                    downloaded_total += 1\n",
    "                    percentage = (downloaded_total / total_papers) * 100\n",
    "                    print(f'\\r {downloaded_total}/{total_papers} ({percentage:.1f}%) - {subject}', end='', flush=True)\n",
    "                \n",
    "                self.download_papers(cat, subject, limit, progress_callback=update_progress)\n",
    "        \n",
    "        print(f'\\n{downloaded_total}/{total_papers}')\n",
    "\n",
    "downloader = ArxivDownloader(base_dir=\"arxiv_paper_dataset\")\n",
    "downloader.download_dataset(papers_per_subject=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e0f32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 350/350 (100.0%) - statisticsience\n",
      " File saved: dataset_papers.json\n",
      "Articles processed: 350\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "2: This cell processes the downloaded arXiv papers to create a structured JSON dataset. \n",
    "It extracts metadata (title and abstract) from the text files and full paper content from PDFs.\n",
    "The code applies text cleaning to remove LaTeX formatting and mathematical expressions, then validates that papers meet minimum length requirements. \n",
    "It processes up to 50 papers per subject category, filtering out any papers with incomplete metadata or insufficient content.'''\n",
    "\n",
    "\n",
    "class SuppressOutput:\n",
    "    def __enter__(self):\n",
    "        self.old_stdout = sys.stdout\n",
    "        self.old_stderr = sys.stderr\n",
    "        sys.stdout = StringIO()\n",
    "        sys.stderr = StringIO()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args):\n",
    "        sys.stdout = self.old_stdout\n",
    "        sys.stderr = self.old_stderr\n",
    "\n",
    "\n",
    "\n",
    "def extract_metadata_from_txt(txt_path):\n",
    "    metadata = {'title': '', 'abstract': ''}\n",
    "    \n",
    "    try:\n",
    "        with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        title_match = re.search(r'Title[:\\s]+([^\\n]+)', content, re.IGNORECASE)\n",
    "        if title_match:\n",
    "            metadata['title'] = title_match.group(1).strip()\n",
    "        \n",
    "        abstract_match = re.search(r'ABSTRACT[:\\s]+([^\\n]+.*?)(?:\\n\\n|\\Z)', content, re.DOTALL | re.IGNORECASE)\n",
    "        if abstract_match:\n",
    "            metadata['abstract'] = abstract_match.group(1).strip()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        with SuppressOutput():\n",
    "            elements = partition_pdf(filename=pdf_path, strategy=\"auto\", languages=[\"eng\"])\n",
    "            text = \"\\n\\n\".join([el.text for el in elements])\n",
    "        return text if len(text) > 100 else \"\"\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\{[^}]*\\}', '', text)\n",
    "    text = re.sub(r'\\$[^$]+\\$', '', text)\n",
    "    text = clean_extra_whitespace(text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def find_introduction(text):\n",
    "    patterns = [r'\\b1\\.?\\s+Introduction\\b', r'\\bIntroduction\\b', r'\\bINTRODUCTION\\b']\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            return text[match.start():].strip(), True\n",
    "    return text, False\n",
    "\n",
    "\n",
    "def process_papers(base_path, max_per_subject=50):\n",
    "\n",
    "    subjects = ['biology', 'computer_science', 'economics', 'finance', \n",
    "                'mathematics', 'physics', 'statistics']\n",
    "    \n",
    "    articles = []\n",
    "    article_id = 1\n",
    "    processed_total = 0\n",
    "    total_needed = max_per_subject * len(subjects)\n",
    "    \n",
    "    for subject in subjects:\n",
    "        subject_path = os.path.join(base_path, subject)\n",
    "\n",
    "        if not os.path.exists(subject_path):\n",
    "            continue\n",
    "\n",
    "        all_pdf_files = [f for f in os.listdir(subject_path) if f.endswith('.pdf')]\n",
    "        \n",
    "        subject_count = 0\n",
    "        file_index = 0\n",
    "\n",
    "        while subject_count < max_per_subject and file_index < len(all_pdf_files):\n",
    "            file = all_pdf_files[file_index]\n",
    "            file_index += 1\n",
    "            \n",
    "            pdf_path = os.path.join(subject_path, file)\n",
    "            arxiv_id = os.path.splitext(file)[0]\n",
    "\n",
    "            txt_path = None\n",
    "            for txt_file in os.listdir(subject_path):\n",
    "                if txt_file.endswith('.txt') and arxiv_id in txt_file:\n",
    "                    txt_path = os.path.join(subject_path, txt_file)\n",
    "                    break\n",
    "            \n",
    "            if not txt_path:\n",
    "                continue\n",
    "            \n",
    "            metadata = extract_metadata_from_txt(txt_path)\n",
    "            if not metadata['title'] or not metadata['abstract']:\n",
    "                continue\n",
    "            \n",
    "            full_text = extract_text_from_pdf(pdf_path)\n",
    "            if len(full_text) < 500:\n",
    "                continue\n",
    "            \n",
    "            cleaned = clean_text(full_text)\n",
    "            final_text, found_intro = find_introduction(cleaned)\n",
    "            \n",
    "            if not found_intro or len(final_text) < 500:\n",
    "                continue\n",
    "            \n",
    "            article = {\n",
    "                'id': f'id{article_id}',\n",
    "                'title': metadata['title'],\n",
    "                'description': metadata['abstract'],\n",
    "                'full_text': final_text,\n",
    "                'subject': subject,\n",
    "                'version': [],\n",
    "                'type': 'paper',\n",
    "                'folder': [f\"c{random.randint(1,100)}\" for _ in range(random.randint(1,5))]\n",
    "            }\n",
    "            \n",
    "            articles.append(article)\n",
    "            article_id += 1\n",
    "            subject_count += 1\n",
    "            processed_total += 1\n",
    "            \n",
    "            percentage = (processed_total / total_needed) * 100\n",
    "            print(f'\\r {processed_total}/{total_needed} ({percentage:.1f}%) - {subject}', end='', flush=True)\n",
    "    \n",
    "    return articles\n",
    "\n",
    "def create_json_dataset(output_file=\"dataset_papers.json\", max_per_subject=50):\n",
    "    \n",
    "    base_path = \"arxiv_paper_dataset\"\n",
    "    \n",
    "    articles = process_papers(base_path, max_per_subject)\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(articles, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f'\\n File saved: {output_file}')\n",
    "    print(f'Articles processed: {len(articles)}\\n')\n",
    "\n",
    "\n",
    "create_json_dataset(output_file=\"dataset_papers.json\", max_per_subject=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e66eb5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total Documents: 2100 (350 orig + 1750 versions)\n",
      " Total Pairs: 193008 [Version: 5250, Similar: 26082, Unrelated: 161676]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''3: \n",
    "For each original paper, it creates 5 modified versions by randomly removing 10-50% of the text from different portions \n",
    "It then organizes all documents (originals + versions) by random folder tags and generates document pairs with three types of labels: \n",
    "\"versions\" (same paper with modifications), \"similar\" (different papers from the same subject), and \"unrelated\" (papers from different subjects).'''\n",
    "\n",
    "class DocumentPair:\n",
    "    def __init__(self, doc1_id, doc2_id, doc1_text, doc2_text, label, label_name):\n",
    "        self.doc1_id = doc1_id\n",
    "        self.doc2_id = doc2_id\n",
    "        self.doc1_text = doc1_text\n",
    "        self.doc2_text = doc2_text\n",
    "        self.label = label\n",
    "        self.label_name = label_name\n",
    "\n",
    "def remove_text_portion(text, percentage):\n",
    "    text_len = len(text)\n",
    "    remove_len = int(text_len * percentage / 100)\n",
    "    mode = random.choice(['start', 'end', 'middle', 'start_end'])\n",
    "    \n",
    "    if mode == 'start': \n",
    "        return text[remove_len:]\n",
    "    elif mode == 'end': \n",
    "        return text[:-remove_len]\n",
    "    elif mode == 'middle':\n",
    "        start = (text_len - remove_len) // 2\n",
    "        return text[:start] + text[start + remove_len:]\n",
    "    else: \n",
    "        half_remove = remove_len // 2\n",
    "        return text[half_remove:-half_remove]\n",
    "\n",
    "def get_next_id(articles):\n",
    "    max_id = 0\n",
    "    for article in articles:\n",
    "        article_id = article.get('id', '')\n",
    "        if article_id.startswith('id'):\n",
    "            max_id = max(max_id, int(article_id[2:]))\n",
    "    return max_id + 1\n",
    "\n",
    "def determine_label(doc1, doc2):\n",
    "    if doc2['id'] in doc1.get('version', []) or doc1['id'] in doc2.get('version', []):\n",
    "        return 'versions'\n",
    "    if doc1.get('subject') == doc2.get('subject') and doc1.get('subject') is not None:\n",
    "        return 'similar'\n",
    "    return 'unrelated'\n",
    "\n",
    "def process_pipeline(input_file=\"dataset_papers.json\"):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        articles = json.load(f)\n",
    "    \n",
    "    num_original = len(articles)\n",
    "    new_versions = []\n",
    "    next_id = get_next_id(articles)\n",
    "\n",
    "    for article in articles:\n",
    "        article.setdefault('version', [])\n",
    "        article.setdefault('folder', [])\n",
    "        \n",
    "        version_ids = []\n",
    "        temp_versions = []\n",
    "        \n",
    "        for i in range(5):\n",
    "            percentage = random.randint(10, 50)\n",
    "            v_id = f'id{next_id}'\n",
    "            version_ids.append(v_id)\n",
    "            \n",
    "            version = {\n",
    "                'id': v_id,\n",
    "                'title': article['title'],\n",
    "                'description': article.get('description', ''),\n",
    "                'full_text': remove_text_portion(article['full_text'], percentage),\n",
    "                'subject': article['subject'],\n",
    "                'version': [article['id']] + [vid for vid in version_ids if vid != v_id],\n",
    "                'type': article['type'],\n",
    "                'folder': article['folder'].copy()\n",
    "            }\n",
    "            temp_versions.append(version)\n",
    "            next_id += 1\n",
    "        \n",
    "        article['version'].extend(version_ids)\n",
    "        \n",
    "        for version in temp_versions:\n",
    "            version['version'] = [article['id']] + [vid for vid in version_ids if vid != version['id']]\n",
    "        \n",
    "        new_versions.extend(temp_versions)\n",
    "    \n",
    "    all_docs = articles + new_versions\n",
    "    \n",
    "    by_folder = {}\n",
    "    for doc in all_docs:\n",
    "        for folder in doc.get('folder', []):\n",
    "            by_folder.setdefault(folder, []).append(doc)\n",
    "    \n",
    "    LABEL_MAP = {'unrelated': 0, 'similar': 1, 'versions': 2}\n",
    "    pairs = []\n",
    "    seen_pairs = set()\n",
    "    \n",
    "    for docs in by_folder.values():\n",
    "        for d1, d2 in combinations(docs, 2):\n",
    "            pair_key = tuple(sorted([d1['id'], d2['id']]))\n",
    "            if pair_key not in seen_pairs:\n",
    "                seen_pairs.add(pair_key)\n",
    "                label_name = determine_label(d1, d2)\n",
    "                pairs.append(DocumentPair(\n",
    "                    d1['id'], d2['id'], \n",
    "                    d1.get('full_text', ''), d2.get('full_text', ''),\n",
    "                    LABEL_MAP[label_name], label_name\n",
    "                ))\n",
    "    \n",
    "    random.shuffle(pairs)\n",
    "\n",
    "    with open(\"dataset_papers_versions.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_docs, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    with open(\"document_pairs.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump([{\n",
    "            'doc1_id': p.doc1_id, \n",
    "            'doc2_id': p.doc2_id, \n",
    "            'label': p.label, \n",
    "            'label_name': p.label_name\n",
    "        } for p in pairs], f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    v_c = sum(1 for p in pairs if p.label_name == 'versions')\n",
    "    s_c = sum(1 for p in pairs if p.label_name == 'similar')\n",
    "    u_c = sum(1 for p in pairs if p.label_name == 'unrelated')\n",
    "    \n",
    "    print(f' Total Documents: {len(all_docs)} ({num_original} orig + {len(new_versions)} versions)')\n",
    "    print(f' Total Pairs: {len(pairs)} [Version: {v_c}, Similar: {s_c}, Unrelated: {u_c}]\\n')\n",
    "    \n",
    "    return all_docs, pairs\n",
    "\n",
    "all_docs, pairs = process_pipeline(\"dataset_papers.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56c860ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 154406 samples | Val: 38602 samples\n",
      "Train batches: 19301 | Val batches: 4826\n"
     ]
    }
   ],
   "source": [
    "'''4:\n",
    "The prepare_datasets function splits the document pairs into training (80%) and validation (20%) sets \n",
    "using stratified sampling to maintain label distribution, then creates DataLoader objects with batch size 8 for efficient model training '''\n",
    "\n",
    "class DocumentPairDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.texts_1 = [p.doc1_text for p in pairs]\n",
    "        self.texts_2 = [p.doc2_text for p in pairs]\n",
    "        self.labels = torch.tensor([p.label for p in pairs], dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts_1[idx], self.texts_2[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts_1, texts_2, labels = zip(*batch)\n",
    "    return {\n",
    "        'texts_1': list(texts_1),\n",
    "        'texts_2': list(texts_2),\n",
    "        'labels': torch.stack(labels)\n",
    "    }\n",
    "\n",
    "\n",
    "def prepare_datasets(pairs, test_size=0.2, batch_size=8):\n",
    "    train_pairs, val_pairs = train_test_split(\n",
    "        pairs, \n",
    "        test_size=test_size, \n",
    "        random_state=42,\n",
    "        stratify=[p.label for p in pairs]\n",
    "    )\n",
    "    \n",
    "    print(f\"Train: {len(train_pairs)} samples | Val: {len(val_pairs)} samples\")\n",
    "\n",
    "    train_dataset = DocumentPairDataset(train_pairs)\n",
    "    val_dataset = DocumentPairDataset(val_pairs)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    print(f\"Train batches: {len(train_loader)} | Val batches: {len(val_loader)}\")\n",
    "    \n",
    "    return train_dataset, val_dataset, train_loader, val_loader\n",
    "\n",
    "train_dataset, val_dataset, train_loader, val_loader = prepare_datasets(pairs=pairs, test_size=0.2, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50cec710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "'''5:\n",
    "This cell defines a Siamese neural network that classifies document pairs into three categories (unrelated, similar, or versions). \n",
    "It uses a pre-trained sentence transformer to encode both documents, then combines their embeddings with difference and product features before \n",
    "passing them through a classifier.'''\n",
    "\n",
    "class SiameseDocumentClassifier(nn.Module):\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\", num_classes=3, \n",
    "                 dropout=0.1, class_weights=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = SentenceTransformer(model_name)\n",
    "        self.hidden_size = self.encoder.get_sentence_embedding_dimension()\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size * 4, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.register_buffer('class_weights', class_weights)\n",
    "        \n",
    "    \n",
    "    def forward(self, texts_1, texts_2, labels=None):\n",
    "        emb1 = self.encoder.encode(texts_1, convert_to_tensor=True, show_progress_bar=False)\n",
    "        emb2 = self.encoder.encode(texts_2, convert_to_tensor=True, show_progress_bar=False)\n",
    "        \n",
    "        #features for classifier\n",
    "        diff = torch.abs(emb1 - emb2)\n",
    "        prod = emb1 * emb2\n",
    "        features = torch.cat([emb1, emb2, diff, prod], dim=1)\n",
    "        \n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "\n",
    "def compute_class_weights(train_loader, device):\n",
    "    all_labels = []\n",
    "    for batch in train_loader:\n",
    "        all_labels.extend(batch['labels'].numpy())\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    unique = np.unique(all_labels)\n",
    "    weights = compute_class_weight('balanced', classes=unique, y=all_labels)\n",
    "    class_weights = torch.FloatTensor(weights).to(device)\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "class_weights = compute_class_weights(train_loader, device)\n",
    "\n",
    "siamese_model = SiameseDocumentClassifier(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    num_classes=3,\n",
    "    dropout=0.1,\n",
    "    class_weights=class_weights\n",
    ")\n",
    "siamese_model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7b2f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 7 epochs | LR: 2e-05\n",
      "\n",
      "Epoch 1/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 19301/19301 [2:17:39<00:00,  2.34it/s]  \n",
      "Validation: 100%|██████████| 4826/4826 [30:57<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5443 | Train Acc: 0.8416\n",
      "Val Loss:   0.3832 | Val Acc:   0.8330\n",
      "⭐ Best model saved!\n",
      "\n",
      "Epoch 2/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 19301/19301 [3:07:46<00:00,  1.71it/s]  \n",
      "Validation: 100%|██████████| 4826/4826 [53:23<00:00,  1.51it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3207 | Train Acc: 0.8834\n",
      "Val Loss:   0.2805 | Val Acc:   0.8949\n",
      "⭐ Best model saved!\n",
      "\n",
      "Epoch 3/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 9128/19301 [57:31<1:00:32,  2.80it/s]"
     ]
    }
   ],
   "source": [
    "'''6:\n",
    "This cell trains the Siamese model over 7 epochs using the AdamW optimizer. \n",
    "It tracks training and validation loss/accuracy for each epoch, automatically reduces the learning rate when validation loss stops improving\n",
    "and saves the best model based on validation accuracy. '''\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(batch['texts_1'], batch['texts_2'], labels)\n",
    "        \n",
    "        loss = outputs['loss']\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        predictions = torch.argmax(outputs['logits'], dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def validate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(batch['texts_1'], batch['texts_2'], labels)\n",
    "            \n",
    "            total_loss += outputs['loss'].item()\n",
    "            predictions = torch.argmax(outputs['logits'], dim=1)\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = sum(p == l for p, l in zip(all_preds, all_labels)) / len(all_labels) if all_labels else 0.0\n",
    "    return avg_loss, accuracy, all_preds, all_labels\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, epochs=5, lr=2e-5):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    print(f\"Training: {epochs} epochs | LR: {lr}\")\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, device)\n",
    "        val_loss, val_acc, val_preds, val_labels = validate(model, val_loader, device)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(\"Best model saved!\")\n",
    "    \n",
    "    print(f\"\\nBest Validation Accuracy: {best_val_acc:.4f}\")\n",
    "    \n",
    "    label_names = ['unrelated', 'similar', 'versions']\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(val_labels, val_preds, target_names=label_names, digits=4))\n",
    "    \n",
    "    return history, val_preds, val_labels\n",
    "\n",
    "\n",
    "history, val_preds, val_labels = train_model(\n",
    "    model=siamese_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    epochs=7,\n",
    "    lr=2e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b606cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''7: CONFUSION MATRIX & METRICS\n",
    "Generates confusion matrix, detailed metrics per class, and error analysis\n",
    "using the best trained model from the previous cell.'''\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names=['unrelated', 'similar', 'versions']):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=False, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    \n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(class_names)):\n",
    "            count = cm[i, j]\n",
    "            percent = cm_percent[i, j]\n",
    "            color = 'white' if cm[i, j] > cm.max() / 2 else 'black'\n",
    "            plt.text(j + 0.5, i + 0.5, f'{count}\\n({percent:.1f}%)',\n",
    "                    ha='center', va='center', color=color, \n",
    "                    fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.title('Confusion Matrix\\n(Count and Row %)', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return cm\n",
    "\n",
    "\n",
    "def analyze_results(val_labels, val_preds):\n",
    "    class_names = ['unrelated', 'similar', 'versions']\n",
    "\n",
    "    \n",
    "    # Classification report\n",
    "    report = classification_report(val_labels, val_preds, \n",
    "                                   target_names=class_names, \n",
    "                                   digits=4, \n",
    "                                   output_dict=True)\n",
    "    \n",
    "    # Print per-class metrics\n",
    "    for class_name in class_names:\n",
    "        metrics = report[class_name]\n",
    "        print(f\"\\nClass: {class_name.upper()}\")\n",
    "        print(f\"  Precision: {metrics['precision']:.4f} ({metrics['precision']*100:.2f}%)\")\n",
    "        print(f\"  Recall:    {metrics['recall']:.4f} ({metrics['recall']*100:.2f}%)\")\n",
    "        print(f\"  F1-Score:  {metrics['f1-score']:.4f} ({metrics['f1-score']*100:.2f}%)\")\n",
    "        print(f\"  Support:   {int(metrics['support'])} samples\")\n",
    "    \n",
    "    # Overall metrics\n",
    "\n",
    "    print(\"OVERALL METRICS\")\n",
    "\n",
    "    print(f\"Accuracy:        {report['accuracy']:.4f} ({report['accuracy']*100:.2f}%)\")\n",
    "    print(f\"Macro Avg F1:    {report['macro avg']['f1-score']:.4f}\")\n",
    "    print(f\"Weighted Avg F1: {report['weighted avg']['f1-score']:.4f}\")\n",
    "    \n",
    "    # Error analysis\n",
    "    errors = np.array(val_labels) != np.array(val_preds)\n",
    "    n_errors = errors.sum()\n",
    "    n_total = len(val_labels)\n",
    "\n",
    "    print(\"ERROR ANALYSIS\")\n",
    "\n",
    "    print(f\"\\nTotal errors: {n_errors}/{n_total} ({n_errors/n_total*100:.2f}%)\")\n",
    "\n",
    "    if n_errors > 0:\n",
    "        error_types = {}\n",
    "        for true_label, pred_label in zip(np.array(val_labels)[errors], \n",
    "                                          np.array(val_preds)[errors]):\n",
    "            key = f\"{class_names[true_label]} → {class_names[pred_label]}\"\n",
    "            error_types[key] = error_types.get(key, 0) + 1\n",
    "        \n",
    "        print(\"\\nError types (frequency):\")\n",
    "        for error_type, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True):\n",
    "            pct = count / n_errors * 100\n",
    "            print(f\"  {error_type:<30} {count:>4} ({pct:>5.1f}%)\")\n",
    "\n",
    "    cm = plot_confusion_matrix(val_labels, val_preds, class_names)\n",
    "    return report, cm\n",
    "\n",
    "\n",
    "report, cm = analyze_results(val_labels, val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c920e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embeddings: 100%|██████████| 2100/2100 [04:18<00:00,  8.12it/s]\n"
     ]
    }
   ],
   "source": [
    "'''8:\n",
    "This cell generates embeddings for all documents in the dataset using the trained Siamese model's encoder. \n",
    "For each document, it creates separate embeddings for the title, abstract (description)'''\n",
    "\n",
    "def get_single_embedding(text, model, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding_tensor = model.encoder.encode([text], convert_to_tensor=True, show_progress_bar=False)\n",
    "        embedding_list = embedding_tensor[0].cpu().numpy().tolist()\n",
    "        return embedding_list\n",
    "\n",
    "with open(\"dataset_papers_versions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "\n",
    "for article in tqdm(dataset, desc=\"Embeddings\"):\n",
    "    title = article.get(\"title\")\n",
    "    description = article.get(\"description\")\n",
    "    full_text = article.get(\"full_text\")\n",
    "    \n",
    "    if title and title.strip():\n",
    "        article[\"embedding_title\"] = get_single_embedding(title, siamese_model, device)\n",
    "    \n",
    "    if description and description.strip():\n",
    "        article[\"embedding_description\"] = get_single_embedding(description, siamese_model, device)\n",
    "    \n",
    "    if full_text and full_text.strip():\n",
    "        article[\"embedding_full_text\"] = get_single_embedding(full_text, siamese_model, device)\n",
    "\n",
    "with open(\"dataset_papers_with_embeddings.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dataset, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de2791b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''9: \n",
    "This cell implements document clustering within folders using graph-based community detection. \n",
    "It builds a weighted similarity graph for documents in each folder based on their embeddings, connecting documents with cosine similarity above 0.70 \n",
    "(keeping top 8 connections per document). \n",
    "The Louvain algorithm then detects communities in this graph to group similar documents into clusters.'''  \n",
    "\n",
    "def load_dataset_simple(embeddings_path):\n",
    "    with open(embeddings_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        documents = json.load(f)\n",
    "    \n",
    "    for doc in documents:\n",
    "        doc[\"id\"] = doc.get(\"id\") \n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "def build_folder_weighted_graph(documents, folder_name, embedding_key, \n",
    "                                min_similarity=0.70, top_k=8):\n",
    "\n",
    "    folder_docs = [d for d in documents if folder_name in d[\"folder\"]]\n",
    "    \n",
    "    valid_docs, valid_emb = [], []\n",
    "    for doc in folder_docs:\n",
    "        emb = doc.get(embedding_key)\n",
    "        if isinstance(emb, list):\n",
    "            valid_docs.append(doc)\n",
    "            valid_emb.append(np.array(emb))\n",
    "    \n",
    "    emb_matrix = np.vstack(valid_emb)\n",
    "    sim_matrix = cosine_similarity(emb_matrix)\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    for doc in valid_docs:\n",
    "        G.add_node(doc[\"id\"], **doc)\n",
    "    \n",
    "    for i in range(len(valid_docs)):\n",
    "        sims = sim_matrix[i].copy()\n",
    "        sims[i] = -1 \n",
    "        \n",
    "\n",
    "        top_indices = np.argsort(sims)[::-1][:top_k]\n",
    "\n",
    "        for j in top_indices:\n",
    "            if sims[j] >= min_similarity:\n",
    "                if not G.has_edge(valid_docs[i][\"id\"], valid_docs[j][\"id\"]):\n",
    "                    G.add_edge(valid_docs[i][\"id\"], valid_docs[j][\"id\"], weight=float(sims[j]))\n",
    "    \n",
    "    return G, valid_docs\n",
    "\n",
    "\n",
    "def apply_louvain_clustering(G, resolution=0.8):\n",
    "    communities = louvain_communities(G, weight=\"weight\", resolution=resolution)\n",
    "    partition = {node: cid for cid, nodes in enumerate(communities) for node in nodes}\n",
    "    \n",
    "    clusters = {}\n",
    "    for node, cid in partition.items():\n",
    "        clusters.setdefault(cid, []).append(node)\n",
    "    \n",
    "    return partition, clusters\n",
    "\n",
    "\n",
    "def create_cluster_json(documents, folder_name, clusters):\n",
    "    doc_by_id = {doc[\"id\"]: doc for doc in documents}\n",
    "    \n",
    "    result = {\"folder\": folder_name, \"num_clusters\": len(clusters)}\n",
    "    \n",
    "    sorted_clusters = sorted(clusters.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "    \n",
    "    for idx, (cid, doc_ids) in enumerate(sorted_clusters, 1):\n",
    "        result[f\"cluster_{idx}\"] = doc_ids\n",
    "        result[f\"file_names_cluster_{idx}\"] = [\n",
    "            doc_by_id[d].get(\"id\", d) for d in doc_ids\n",
    "        ]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3f0fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''10:\n",
    "This cell defines functions to evaluate clustering quality using multiple metrics.'''\n",
    "\n",
    "def compute_embedding_metrics(embeddings_matrix, labels):\n",
    "    metrics = {}\n",
    "    \n",
    "\n",
    "    distances = cosine_distances(embeddings_matrix)\n",
    "    sil_score = silhouette_score(distances, labels, metric='precomputed')\n",
    "    metrics[\"silhouette_cosine\"] = float(sil_score)\n",
    "\n",
    "    db_score = davies_bouldin_score(embeddings_matrix, labels)\n",
    "    metrics[\"davies_bouldin\"] = float(db_score)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compute_conductance(G, partition):\n",
    "    communities = {}\n",
    "    for node, comm_id in partition.items():\n",
    "        communities.setdefault(comm_id, set()).add(node)\n",
    "    \n",
    "    conductances = []\n",
    "    \n",
    "    for comm_id, nodes in communities.items():\n",
    "        if len(nodes) < 2:\n",
    "            continue\n",
    "        \n",
    "        internal_edges = 0\n",
    "        external_edges = 0\n",
    "        \n",
    "        for node in nodes:\n",
    "            for neighbor in G.neighbors(node):\n",
    "                if neighbor in nodes:\n",
    "                    internal_edges += 1\n",
    "                else:\n",
    "                    external_edges += 1\n",
    "        \n",
    "        internal_edges = internal_edges / 2\n",
    "        total_edges = internal_edges + external_edges\n",
    "        \n",
    "        if total_edges > 0:\n",
    "            cond = external_edges / total_edges\n",
    "            conductances.append(cond)\n",
    "    \n",
    "    return {\n",
    "        \"conductance_mean\": float(np.mean(conductances)),\n",
    "        \"conductance_std\": float(np.std(conductances)),\n",
    "        \"conductance_min\": float(np.min(conductances)),\n",
    "        \"conductance_max\": float(np.max(conductances))\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_folder_clustering(G, partition, valid_docs, embedding_key=\"embedding_full_text\"):\n",
    "    results = {}\n",
    "    \n",
    "    results[\"graph\"] = compute_conductance(G, partition)\n",
    "    \n",
    "    doc_by_id = {doc[\"id\"]: doc for doc in valid_docs}\n",
    "    embeddings_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for node_id in G.nodes():\n",
    "        if node_id in doc_by_id and node_id in partition:\n",
    "            emb = doc_by_id[node_id].get(embedding_key)\n",
    "            if emb:\n",
    "                embeddings_list.append(emb)\n",
    "                labels_list.append(partition[node_id])\n",
    "    \n",
    "    embeddings_matrix = np.array(embeddings_list)\n",
    "    labels_array = np.array(labels_list)\n",
    "    results[\"embedding\"] = compute_embedding_metrics(embeddings_matrix, labels_array)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70738108",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''11:\n",
    "This cell runs the complete clustering pipeline across all folders in the dataset. \n",
    "For each folder, it builds a similarity graph, applies Louvain clustering with specified parameters \n",
    "(minimum similarity 0.25, top 6 connections per document, resolution 1), and evaluates the clustering quality.\n",
    "It computes and displays average clustering metrics'''\n",
    "\n",
    "def run_complete_pipeline(embeddings_json, output_json,\n",
    "                          min_similarity=0.70, top_k=8, resolution=0.8,\n",
    "                          test_folder=None):\n",
    "\n",
    "    \n",
    "    documents = load_dataset_simple(embeddings_json)\n",
    "    all_folders = sorted({f for d in documents for f in d.get(\"folder\", [])})\n",
    "    \n",
    "    if test_folder:\n",
    "        all_folders = [test_folder]\n",
    "    \n",
    "    all_results = {}\n",
    "    metrics_results = {}\n",
    "    \n",
    "    print(f\"Processing {len(all_folders)} folders\")\n",
    "    \n",
    "    for folder in tqdm(all_folders, desc=\"Clustering\"):\n",
    "        G, valid_docs = build_folder_weighted_graph(\n",
    "            documents, folder, \"embedding_full_text\",\n",
    "            min_similarity=min_similarity,\n",
    "            top_k=top_k\n",
    "        )\n",
    "        \n",
    "        if G is None or valid_docs is None or len(valid_docs) < 2:\n",
    "            continue\n",
    "        \n",
    "        partition, clusters = apply_louvain_clustering(G, resolution=resolution)\n",
    "        all_results[folder] = create_cluster_json(documents, folder, clusters)\n",
    "        metrics_results[folder] = evaluate_folder_clustering(G, partition, valid_docs)\n",
    "    \n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Saved clustering results to {output_json}\")\n",
    "    \n",
    "    return all_results, metrics_results\n",
    "\n",
    "\n",
    "def summarize_metrics(metrics_results):\n",
    "    rows = []\n",
    "    \n",
    "    for folder, metrics in metrics_results.items():\n",
    "        row = {\"folder\": folder}\n",
    "        \n",
    "        for key, value in metrics[\"graph\"].items():\n",
    "            row[f\"graph_{key}\"] = value\n",
    "        \n",
    "        for key, value in metrics[\"embedding\"].items():\n",
    "            row[f\"embedding_{key}\"] = value\n",
    "        \n",
    "        rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    summary = pd.DataFrame({\n",
    "        \"mean\": df.drop(columns=\"folder\").mean(),\n",
    "        \"std\": df.drop(columns=\"folder\").std()\n",
    "    })\n",
    "    \n",
    "    print(\"\\n=== Average metrics across folders ===\")\n",
    "    print(summary)\n",
    "    \n",
    "    return df, summary\n",
    "\n",
    "\n",
    "all_results, metrics_results = run_complete_pipeline(\n",
    "    embeddings_json=\"dataset_papers_with_embeddings.json\",\n",
    "    output_json=\"all_folders_fewer_clusters.json\",\n",
    "    min_similarity=0.25,\n",
    "    top_k=6,\n",
    "    resolution=1\n",
    ")\n",
    "\n",
    "df_metrics, summary_metrics = summarize_metrics(metrics_results)\n",
    "\n",
    "print(f\"\\nProcessed {len(all_results)} folders\")\n",
    "print(f\"Total clusters: {sum(r['num_clusters'] for r in all_results.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81abe0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''12:\n",
    "This cell evaluates clustering quality using purity metrics.\n",
    "For each cluster, it calculates purity as the fraction of documents belonging to the most common subject. \n",
    "It then reports the percentage of clusters achieving purity thresholds of ≥50%, ≥70%, and ≥90%, both per folder and averaged across all folders.''' \n",
    "\n",
    "def build_id_to_label_map(dataset_json_path):\n",
    "    \"\"\"Map document_id -> subject (ground truth)\"\"\"\n",
    "    with open(dataset_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "    \n",
    "    id_to_label = {}\n",
    "    for item in dataset:\n",
    "        subject = item.get(\"subject\")\n",
    "        for doc_id in item.get(\"version\", []):\n",
    "            id_to_label[doc_id] = subject\n",
    "    \n",
    "    return id_to_label\n",
    "\n",
    "\n",
    "def compute_cluster_purities(folder_data, id_to_label):\n",
    "    \"\"\"Compute purity for each cluster in a folder\"\"\"\n",
    "    purities = []\n",
    "    \n",
    "    for key, ids in folder_data.items():\n",
    "        if not key.startswith(\"cluster_\"):\n",
    "            continue\n",
    "        \n",
    "        labels = [id_to_label[i] for i in ids if i in id_to_label]\n",
    "        if not labels:\n",
    "            continue\n",
    "        \n",
    "        most_common = Counter(labels).most_common(1)[0][1]\n",
    "        purities.append(most_common / len(labels))\n",
    "    \n",
    "    return purities\n",
    "\n",
    "\n",
    "def compute_purity_all_folders(cluster_json_path, dataset_json_path):\n",
    "    \"\"\"Compute purity per folder and global average\"\"\"\n",
    "    \n",
    "    with open(cluster_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        all_clusters = json.load(f)\n",
    "    \n",
    "    id_to_label = build_id_to_label_map(dataset_json_path)\n",
    "    \n",
    "    folder_results = {\n",
    "        \"per_folder\": {},\n",
    "        \"average\": {}\n",
    "    }\n",
    "\n",
    "    purity_50_vals = []\n",
    "    purity_70_vals = []\n",
    "    purity_90_vals = []\n",
    "    \n",
    "    for folder, folder_data in all_clusters.items():\n",
    "        purities = compute_cluster_purities(folder_data, id_to_label)\n",
    "        if not purities:\n",
    "            continue\n",
    "        \n",
    "        N = len(purities)\n",
    "        \n",
    "        p50 = sum(p >= 0.50 for p in purities) / N\n",
    "        p70 = sum(p >= 0.70 for p in purities) / N\n",
    "        p90 = sum(p >= 0.90 for p in purities) / N\n",
    "        \n",
    "        folder_results[\"per_folder\"][folder] = {\n",
    "            \"num_clusters\": N,\n",
    "            \"purity_50_frac\": p50,\n",
    "            \"purity_70_frac\": p70,\n",
    "            \"purity_90_frac\": p90\n",
    "        }\n",
    "        \n",
    "        purity_50_vals.append(p50)\n",
    "        purity_70_vals.append(p70)\n",
    "        purity_90_vals.append(p90)\n",
    "    \n",
    "    folder_results[\"average\"] = {\n",
    "        \"num_folders\": len(folder_results[\"per_folder\"]),\n",
    "        \"purity_50_mean\": float(np.mean(purity_50_vals)),\n",
    "        \"purity_50_std\": float(np.std(purity_50_vals)),\n",
    "        \"purity_70_mean\": float(np.mean(purity_70_vals)),\n",
    "        \"purity_70_std\": float(np.std(purity_70_vals)),\n",
    "        \"purity_90_mean\": float(np.mean(purity_90_vals)),\n",
    "        \"purity_90_std\": float(np.std(purity_90_vals)),\n",
    "    }\n",
    "    \n",
    "    return folder_results\n",
    "\n",
    "\n",
    "def print_purity_results(results):\n",
    "    \"\"\"Print purity statistics\"\"\"\n",
    "    avg = results[\"average\"]\n",
    "    \n",
    "\n",
    "    print(\"\\n AVERAGE PURITY ACROSS ALL FOLDERS\")\n",
    "\n",
    "    print(f\"Number of folders: {avg['num_folders']}\")\n",
    "    print(f\"\\nClusters with purity:\")\n",
    "    print(f\"  ≥ 50%: {avg['purity_50_mean']:.2%} ± {avg['purity_50_std']:.2%}\")\n",
    "    print(f\"  ≥ 70%: {avg['purity_70_mean']:.2%} ± {avg['purity_70_std']:.2%}\")\n",
    "    print(f\"  ≥ 90%: {avg['purity_90_mean']:.2%} ± {avg['purity_90_std']:.2%}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "purity_results = compute_purity_all_folders(\n",
    "    cluster_json_path=\"all_folders_fewer_clusters.json\",\n",
    "    dataset_json_path=\"dataset_papers_versions.json\"\n",
    ")\n",
    "\n",
    "print_purity_results(purity_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ffd681",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''13:\n",
    "This cell generates concise titles for documents using Qwen.\n",
    "It takes the first 800 characters, prompts the LLM to create a title of maximum 10 words.\n",
    "It creates concatenated title strings for each cluster by combining the titles of all documents in that cluster... We will use it later\n",
    "Then it generate the embedding of the generated title... We will need it'''\n",
    "\n",
    "def generate_title(text, tokenizer, model, device):\n",
    "    snippet = text[:800] #limit\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Create concise titles. Output ONLY the title.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Create a title (max 10 words):\\n\\n{snippet}\\n\\nTitle:\"}\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=24, do_sample=False, num_beams=4)\n",
    "    \n",
    "    title = tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()\n",
    "    \n",
    "    title = title.split(\"\\n\")[0].strip('\"\\'')\n",
    "    title = \"\".join(c for c in title if c.isalnum() or c in \" -_.,\").strip()\n",
    "    \n",
    "    return title \n",
    "\n",
    "def get_single_embedding(text, model, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding_tensor = model.encoder.encode([text], convert_to_tensor=True, show_progress_bar=False)\n",
    "        embedding_list = embedding_tensor[0].cpu().numpy().tolist()\n",
    "        return embedding_list\n",
    "\n",
    "def run_pipeline(input_json, output_json, cluster_json=None, model_name=\"Qwen/Qwen2.5-1.5B-Instruct\"):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, dtype=torch.float16 if device == \"cuda\" else torch.float32, device_map=\"auto\"\n",
    "    ).eval()\n",
    "    \n",
    "    with open(input_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    id_to_title = {}\n",
    "    \n",
    "    for item in tqdm(data, desc=\"Generating titles and embeddings\"):\n",
    "        text = item.get(\"full_text\", \"\")\n",
    "        \n",
    "        # Generate title\n",
    "        title = generate_title(text, tokenizer, llm_model, device) \n",
    "        item[\"generated_title\"] = title\n",
    "        \n",
    "        # Generate embedding for the generated title\n",
    "        if title and title.strip():\n",
    "            item[\"embedding_generated_title\"] = get_single_embedding(title, siamese_model, device)\n",
    "        \n",
    "        if \"id\" in item:\n",
    "            id_to_title[item[\"id\"]] = title\n",
    "\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    valid = sum(1 for item in data if item.get(\"generated_title\") != \"Untitled\")\n",
    "    print(f\"{valid}/{len(data)} titles | Saved to {output_json}\")\n",
    "    \n",
    "    if cluster_json:\n",
    "        with open(cluster_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            clusters = json.load(f)\n",
    "        \n",
    "        updated = 0\n",
    "        for folder in clusters.values():\n",
    "            if isinstance(folder, dict):\n",
    "                items = list(folder.items())\n",
    "                for key, ids in items:\n",
    "                    if re.match(r\"^cluster_(\\d+)$\", key) and isinstance(ids, list):\n",
    "                        idx = re.match(r\"^cluster_(\\d+)$\", key).group(1)\n",
    "                        titles = [id_to_title.get(i) for i in ids if id_to_title.get(i)]\n",
    "                        folder[f\"testo_completo_cluster_{idx}\"] = \" \".join(titles)\n",
    "                        updated += 1\n",
    "        \n",
    "        with open(cluster_json, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(clusters, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "\n",
    "\n",
    "run_pipeline(\n",
    "    input_json=\"dataset_papers_with_embeddings.json\",\n",
    "    output_json=\"dataset_papers_with_titles.json\",\n",
    "    cluster_json=\"all_folders_fewer_clusters.json\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b1a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''14: \n",
    "This cell generates descriptive names for clusters and parent folders using QWEN\n",
    "It analyzes the document titles within that cluster and creates a name that captures the common theme.'''\n",
    "\n",
    "def generate_cluster_name(text, tokenizer, model, device):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are an expert at analyzing document collections and creating meaningful \"\n",
    "                \"folder names. Your task is to identify the common theme or topic across \"\n",
    "                \"multiple documents and create a concise, descriptive folder name.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Below is a list of document titles that belong together in the same folder.\n",
    "Analyze their common theme and create a descriptive folder name.\n",
    "\n",
    "Document titles:\n",
    "{text}\n",
    "\n",
    "Requirements:\n",
    "- Maximum 6 words\n",
    "- Use hyphens to separate words (e.g., \"machine-learning-applications\")\n",
    "- Be specific and descriptive\n",
    "- Capture the main topic or theme shared by these documents\n",
    "- Use lowercase letters\n",
    "\n",
    "Output ONLY the folder name, nothing else.\n",
    "\n",
    "Folder name:\"\"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(\n",
    "        prompt, return_tensors=\"pt\", truncation=True, max_length=512  \n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=15,  \n",
    "            num_beams=1,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    name = tokenizer.decode(\n",
    "        output[0][inputs[\"input_ids\"].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    ).strip()\n",
    "\n",
    "    name = name.split(\"\\n\")[0].strip('\"\\'').replace(\" \", \"-\")\n",
    "    name = \"\".join(c for c in name if c.isalnum() or c in \"-_\")\n",
    "    while \"--\" in name:\n",
    "        name = name.replace(\"--\", \"-\")\n",
    "    name = name.strip(\"-_\")[:60]\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "def generate_parent_name(texts, tokenizer, model, device):\n",
    "    combined = \"\\n\".join([t[:100] for t in texts[:10]])\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are an expert at analyzing collections of document clusters and creating \"\n",
    "                \"meaningful parent folder names that capture the overarching theme.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Below are summaries of multiple document clusters that belong in the same parent folder.\n",
    "Analyze their common theme and create a descriptive parent folder name.\n",
    "\n",
    "Cluster summaries:\n",
    "{combined}\n",
    "\n",
    "Requirements:\n",
    "- Maximum 6 words\n",
    "- Use hyphens to separate words\n",
    "- Be broad enough to encompass all clusters\n",
    "- Capture the overarching theme\n",
    "- Use lowercase letters\n",
    "\n",
    "Output ONLY the parent folder name, nothing else.\n",
    "\n",
    "Parent folder name:\"\"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(\n",
    "        prompt, return_tensors=\"pt\", truncation=True, max_length=512  \n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=15, \n",
    "            num_beams=1,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    name = tokenizer.decode(\n",
    "        output[0][inputs[\"input_ids\"].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    ).strip()\n",
    "\n",
    "    name = name.split(\"\\n\")[0].strip('\"\\'').replace(\" \", \"-\")\n",
    "    name = \"\".join(c for c in name if c.isalnum() or c in \"-_\")\n",
    "    while \"--\" in name:\n",
    "        name = name.replace(\"--\", \"-\")\n",
    "    name = name.strip(\"-_\")[:60]\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "def add_folder_names(cluster_json, output_json, model_name, device):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float32,  \n",
    "        low_cpu_mem_usage=True  \n",
    "    ).eval()\n",
    "\n",
    "    with open(cluster_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "\n",
    "    total_clusters = 0\n",
    "    for folder_data in data.values():\n",
    "        cluster_idx = 1\n",
    "        while f\"cluster_{cluster_idx}\" in folder_data:\n",
    "            total_clusters += 1\n",
    "            cluster_idx += 1\n",
    "    \n",
    "    print(f\"Total clusters to process: {total_clusters}\")\n",
    "    parent_success = 0\n",
    "\n",
    "    for folder_name, folder_data in tqdm(data.items(), desc=\"Parent folders\"):\n",
    "        texts = []\n",
    "        cluster_idx = 1\n",
    "\n",
    "        while f\"testo_completo_cluster_{cluster_idx}\" in folder_data:\n",
    "            text = folder_data[f\"testo_completo_cluster_{cluster_idx}\"]\n",
    "            if text:\n",
    "                texts.append(text)\n",
    "            cluster_idx += 1\n",
    "\n",
    "        if texts:\n",
    "            parent_name = generate_parent_name(\n",
    "                texts, tokenizer, llm_model, device\n",
    "            )\n",
    "            folder_data[\"parent_folder_name\"] = parent_name\n",
    "            parent_success += 1\n",
    "\n",
    "    named_clusters = 0\n",
    "\n",
    "    with tqdm(total=total_clusters, desc=\"Naming clusters\") as pbar:\n",
    "        for folder_name, folder_data in data.items():\n",
    "            cluster_idx = 1\n",
    "\n",
    "            while f\"cluster_{cluster_idx}\" in folder_data:\n",
    "                text_key = f\"testo_completo_cluster_{cluster_idx}\"\n",
    "                if text_key in folder_data:\n",
    "                    text = folder_data[text_key]\n",
    "                    name = generate_cluster_name(\n",
    "                        text, tokenizer, llm_model, device\n",
    "                    )\n",
    "                    folder_data[f\"cluster_name_{cluster_idx}\"] = name\n",
    "                    named_clusters += 1\n",
    "                    pbar.update(1)\n",
    "\n",
    "                cluster_idx += 1\n",
    "\n",
    "\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "\n",
    "\n",
    "add_folder_names(\n",
    "    cluster_json=\"all_folders_fewer_clusters.json\",\n",
    "    output_json=\"all_folders_with_names.json\",\n",
    "    model_name=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    device= device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fac03a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''15:\n",
    "This cell computes similarity scores between documents using their embeddings. \n",
    "It performs similarity search by comparing title embeddings against full-text embeddings to find the top 6 most similar documents for each paper. '''\n",
    "\n",
    "NUM_RESULTS = 6\n",
    "\n",
    "def compute_similarities(data: List[Dict[str, Any]], query_embedding_key: str, \n",
    "                        target_embedding_key: str, output_key: str, k: int) -> List[Dict[str, Any]]:\n",
    "\n",
    "    print(f\"Query: {query_embedding_key} → Target: {target_embedding_key}\")\n",
    "    \n",
    "    target_matrix = np.array([a[target_embedding_key] for a in data], dtype=float)\n",
    "    \n",
    "    article_ids = [a[\"id\"] for a in data]\n",
    "    article_titles = [a.get(\"title\", \"N/A\") for a in data]\n",
    "    \n",
    "    output_data = data.copy()\n",
    "    \n",
    "    for i in tqdm(range(len(data)), desc=\"Similarity\"):\n",
    "        query_article = output_data[i]\n",
    "        query_id = article_ids[i]\n",
    "        query_vector = np.array(query_article[query_embedding_key], dtype=float).reshape(1, -1)\n",
    "        \n",
    "\n",
    "        similarity_scores = cosine_similarity(query_vector, target_matrix)[0]\n",
    "        top_k_indices = np.argsort(similarity_scores)[::-1][:k]\n",
    "        \n",
    "        results = []\n",
    "        for rank, idx in enumerate(top_k_indices, start=1):\n",
    "            results.append({\n",
    "                \"rank\": rank,\n",
    "                \"is_query_itself\": (query_id == article_ids[idx]),\n",
    "                \"similarity_score\": round(float(similarity_scores[idx]), 4),\n",
    "                \"target_id\": article_ids[idx],\n",
    "                \"target_title\": article_titles[idx],\n",
    "                \"target_subject\": data[idx].get(\"subject\", \"N/A\")\n",
    "            })\n",
    "        \n",
    "        query_article[output_key] = results\n",
    "    \n",
    "    return output_data\n",
    "\n",
    "with open(\"dataset_papers_with_titles.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    articles_with_embeddings = json.load(f)\n",
    "\n",
    "results = compute_similarities(\n",
    "    data=articles_with_embeddings,\n",
    "    query_embedding_key=\"embedding_title\",\n",
    "    target_embedding_key=\"embedding_full_text\",\n",
    "    output_key=\"most_similar_by_title_to_text\",\n",
    "    k=NUM_RESULTS\n",
    ")\n",
    "\n",
    "\n",
    "with open(\"dataset_similarity_title.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nSaved {len(results)} papers to dataset_similarity_title.json\")\n",
    "\n",
    "example = results[0]\n",
    "print(f\"\\nExample: {example.get('title')}\")\n",
    "for res in example[\"most_similar_by_title_to_text\"][:3]:\n",
    "    flag = \" (SELF)\" if res[\"is_query_itself\"] else \"\"\n",
    "    print(f\"  Rank {res['rank']}: {res['similarity_score']:.4f} - {res['target_title']}{flag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450de464",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''16:\n",
    "It does the same as the cell above but it compares abstract embeddings against full-text embeddings''' \n",
    "\n",
    "results_abstract = compute_similarities(\n",
    "    data=articles_with_embeddings,\n",
    "    query_embedding_key=\"embedding_description\",\n",
    "    target_embedding_key=\"embedding_full_text\",\n",
    "    output_key=\"most_similar_by_abstract_to_text\",\n",
    "    k=NUM_RESULTS\n",
    ")\n",
    "\n",
    "with open(\"dataset_similarity_abstract.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_abstract, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nSaved {len(results_abstract)} papers to dataset_similarity_abstract.json\")\n",
    "\n",
    "example = results_abstract[0]\n",
    "print(f\"\\nExample: {example.get('title')}\")\n",
    "for res in example[\"most_similar_by_abstract_to_text\"][:3]:\n",
    "    flag = \" (SELF)\" if res[\"is_query_itself\"] else \"\"\n",
    "    print(f\"  Rank {res['rank']}: {res['similarity_score']:.4f} - {res['target_title']}{flag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e231a8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''17:\n",
    "It does the same as the cells above but it compares generated title against full-text embeddings''' \n",
    "\n",
    "results_abstract = compute_similarities(\n",
    "    data=articles_with_embeddings,\n",
    "    query_embedding_key=\"embedding_generated_title\",\n",
    "    target_embedding_key=\"embedding_full_text\",\n",
    "    output_key=\"most_similar_by_generated_title_to_text\",\n",
    "    k=NUM_RESULTS\n",
    ")\n",
    "\n",
    "with open(\"dataset_similarity_generated_title.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_abstract, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nSaved {len(results_abstract)} papers to dataset_similarity_generated_title.json\")\n",
    "\n",
    "example = results_abstract[0]\n",
    "print(f\"\\nExample: {example.get('title')}\")\n",
    "for res in example[\"most_similar_by_generated_title_to_text\"][:3]:\n",
    "    flag = \" (SELF)\" if res[\"is_query_itself\"] else \"\"\n",
    "    print(f\"  Rank {res['rank']}: {res['similarity_score']:.4f} - {res['target_title']}{flag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be38901",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''18:\n",
    "This cell evaluates the quality of different similarity search methods by scoring retrieved results against ground truth relationships. \n",
    "For each retrieved document, it assigns a score: \n",
    "        1.0 for exact matches or document versions, \n",
    "        0.5 for documents in the same subject category, \n",
    "        0.0 for unrelated documents. \n",
    "It compares three methods (title-based, abstract-based, and generated title-based similarity) \n",
    "and calculates mean scores and standard deviations across all queries. '''\n",
    "\n",
    "def calculate_Irenescore(query_id: str, target_id: str, metadata: dict) -> float:\n",
    "    if query_id == target_id:\n",
    "        return 1.0\n",
    "    \n",
    "    query_meta = metadata.get(query_id, {})\n",
    "    target_meta = metadata.get(target_id, {})\n",
    "\n",
    "    if target_id in query_meta.get('versions', []) or query_id in target_meta.get('versions', []):\n",
    "        return 1.0\n",
    "\n",
    "    if query_meta.get('subject') and query_meta.get('subject') == target_meta.get('subject'):\n",
    "        return 0.5\n",
    "    \n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def evaluate_method(data: list, metadata: dict, similarity_key: str) -> dict:    \n",
    "    scores = []\n",
    "    \n",
    "    for article in data:\n",
    "        query_id = article.get('id')\n",
    "        results = article.get(similarity_key, [])\n",
    "         \n",
    "        article_score = sum(\n",
    "            calculate_Irenescore(query_id, r.get('target_id'), metadata)\n",
    "            for r in results if r.get('target_id')\n",
    "        )\n",
    "        scores.append(article_score)\n",
    "    \n",
    "    return {\n",
    "        'mean': np.mean(scores) if scores else 0.0,\n",
    "        'std': np.std(scores) if scores else 0.0,\n",
    "        'count': len(scores)\n",
    "    }\n",
    "\n",
    "\n",
    "metadata = {\n",
    "    a['id']: {'subject': a.get('subject'), 'versions': a.get('version', [])}\n",
    "    for a in dataset if a.get('id')\n",
    "}\n",
    "\n",
    "\n",
    "with open(\"dataset_similarity_title.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    sim_title = json.load(f)\n",
    "\n",
    "with open(\"dataset_similarity_abstract.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    sim_abstract = json.load(f)\n",
    "\n",
    "with open(\"dataset_similarity_generated_title.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    sim_generated = json.load(f)\n",
    "\n",
    "results = {\n",
    "    'Title → Text': evaluate_method(sim_title, metadata, 'most_similar_by_title_to_text'),\n",
    "    'Abstract → Text': evaluate_method(sim_abstract, metadata, 'most_similar_by_abstract_to_text'),\n",
    "    'Generated Title → Text': evaluate_method(sim_generated, metadata, 'most_similar_by_generated_title_to_text')\n",
    "}\n",
    "\n",
    "\n",
    "for method, res in sorted(results.items(), key=lambda x: x[1]['mean'], reverse=True):\n",
    "    percentage = (res['mean'] / 6.0) * 100\n",
    "    print(f\"\\n{method}\")\n",
    "    print(f\"Score: {res['mean']:.4f} / 6.00 ({percentage:.1f}%)\")\n",
    "    print(f\"Std:   {res['std']:.4f}\")\n",
    "    print(f\"Count: {res['count']}\")\n",
    "\n",
    "output = {\n",
    "    method: {\n",
    "        'mean_score': float(res['mean']),\n",
    "        'percentage': float((res['mean'] / 6.0) * 100),\n",
    "        'std_score': float(res['std']),\n",
    "        'articles_evaluated': res['count']\n",
    "    }\n",
    "    for method, res in results.items()\n",
    "}\n",
    "\n",
    "with open(\"evaluation_metrics.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output, f, indent=2, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
